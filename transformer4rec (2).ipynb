{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-11T05:27:19.954964Z",
     "iopub.status.busy": "2025-12-11T05:27:19.954471Z",
     "iopub.status.idle": "2025-12-11T05:27:21.813256Z",
     "shell.execute_reply": "2025-12-11T05:27:21.812493Z",
     "shell.execute_reply.started": "2025-12-11T05:27:19.954936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:33:00.882882Z",
     "iopub.status.busy": "2025-12-12T18:33:00.882268Z",
     "iopub.status.idle": "2025-12-12T18:33:00.888312Z",
     "shell.execute_reply": "2025-12-12T18:33:00.887477Z",
     "shell.execute_reply.started": "2025-12-12T18:33:00.882850Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/TraningConfig.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "class TrainingConfig:\n",
    "    '''\n",
    "    All configurations required for the models to train\n",
    "    '''\n",
    "\n",
    "    DATA_DIR = Path('output/data')\n",
    "    OUTPUT_DIR = Path('output')\n",
    "    MODEL_DIR = OUTPUT_DIR / \"models_greatlakes\"\n",
    "    VIZ_DIR = OUTPUT_DIR / \"visualizations\"\n",
    "\n",
    "    #? Model Hyperparameters - Enhanced for better performance\n",
    "    EMBEDDING_DIM = 256\n",
    "    HIDDEN_DIM = 512\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 4\n",
    "    DROPOUT = 0.3\n",
    "    MAX_SEQ_LENGTH = 50\n",
    "\n",
    "    #? Training hyperparameters - Optimized\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.0005\n",
    "    NUM_EPOCHS = 1\n",
    "    PATIENCE = 8\n",
    "    WARMUP_EPOCHS = 3\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "\n",
    "    TOP_K_VALUES = [1, 5, 10, 20]\n",
    "\n",
    "    #? System\n",
    "    NUM_WORKERS = 4\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Initialize output directories'''\n",
    "        self.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        self.VIZ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        self.DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:33:07.503541Z",
     "iopub.status.busy": "2025-12-12T18:33:07.502818Z",
     "iopub.status.idle": "2025-12-12T18:33:07.508215Z",
     "shell.execute_reply": "2025-12-12T18:33:07.507531Z",
     "shell.execute_reply.started": "2025-12-12T18:33:07.503518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/config.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For ML models\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For Arrow file export\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    '''\n",
    "    This class holds all the configurations like \n",
    "    input file path, output file path and hyperparameters for the models\n",
    "    '''\n",
    "    MPD_DIR = Path('/kaggle/input/millionplaylistdataset/data')\n",
    "    OUTPUT_DIR = Path('output')\n",
    "    HF_DATASET_PATH = \"/kaggle/input/mpd-audio-features/extracted_audio_features.csv\"\n",
    "    NUM_FILES_TO_PROCESS = 100  #? How many files should we process in the dataset\n",
    "    MIN_PLAYLIST_LENGTH = 5 #? Lower bound for number of songs in the playlist\n",
    "    MAX_PLAYLIST_LENGTH = 200  #? Upper bound for number of songs in the playlist\n",
    "    MIN_SONG_FREQUENCY = 10  #? How many times that song should be present in all playlists in order to be used.\n",
    "\n",
    "    AUDIO_FEATURES = ['danceability', 'energy', 'loudness', 'speechiness', \n",
    "                     'acousticness', 'instrumentalness', 'liveness', \n",
    "                     'valence', 'tempo', 'duration_ms']\n",
    "    \n",
    "    KNN_NEIGHBORS = 20 #? Hyperparameter for NearestNeigbor classifier\n",
    "    RANDOM_SEED = 42 #? For reproduciblity of code\n",
    "    \n",
    "    # Data splits\n",
    "    TRAIN_RATIO = 0.70 #? 70% of data for training\n",
    "    VAL_RATIO = 0.15 #? 15% Data for validation\n",
    "    TEST_RATIO = 0.15 #? 15% data for testing\n",
    "    \n",
    "    # Evaluation\n",
    "    TOP_K_VALUES = [1, 5, 10, 20]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        (self.OUTPUT_DIR / \"models\").mkdir(exist_ok=True)\n",
    "        (self.OUTPUT_DIR / \"visualizations\").mkdir(exist_ok=True)\n",
    "        (self.OUTPUT_DIR / \"data\").mkdir(exist_ok=True)\n",
    "        (self.OUTPUT_DIR / \"metrics\").mkdir(exist_ok=True)\n",
    "\n",
    "    \n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:33:12.465112Z",
     "iopub.status.busy": "2025-12-12T18:33:12.464526Z",
     "iopub.status.idle": "2025-12-12T18:33:12.470897Z",
     "shell.execute_reply": "2025-12-12T18:33:12.470133Z",
     "shell.execute_reply.started": "2025-12-12T18:33:12.465089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/dataloader.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "from config import Config\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class SpotifyDataLoader:\n",
    "    '''\n",
    "    Handles loading dataset and merging spotify MPD audio features\n",
    "    '''\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.playlists = []\n",
    "        self.track_uri_to_id = {}\n",
    "        self.audio_features_df = None\n",
    "\n",
    "    def load_mpd_files(self) -> List[Dict]:\n",
    "        '''\n",
    "        Loading the Spotify MPD Dataset\n",
    "        '''\n",
    "        print('[1/7] Loading the spotify MPD Dataset')\n",
    "        \n",
    "        json_files = sorted(list(self.config.MPD_DIR.glob(\"*.json\")))\n",
    "\n",
    "        if len(json_files) == 0:\n",
    "            raise FileNotFoundError(f'No JSON files found in {self.config.MPD_DIR}')\n",
    "        \n",
    "        files_to_load = json_files[:self.config.NUM_FILES_TO_PROCESS]\n",
    "\n",
    "        print(f'Found {len(json_files)} files, loading {len(files_to_load)} files...')\n",
    "\n",
    "        all_playlists = []\n",
    "\n",
    "        for json_file in tqdm(files_to_load, desc=\"Loading JSON files\"):\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_playlists.extend(data['playlists'])\n",
    "        \n",
    "        print(f\"Loaded {len(all_playlists):,} playlists\")\n",
    "        return all_playlists\n",
    "    \n",
    "\n",
    "    def load_audio_features(self) -> pd.DataFrame:\n",
    "        df = pd.read_csv(self.config.HF_DATASET_PATH)\n",
    "\n",
    "        print(f\"Loaded {len(df):,} tracks with audio features\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "    def preprocess_data(self, playlists: List[Dict], audio_df: pd.DataFrame):\n",
    "        '''\n",
    "        Clean and merge playlist data with audio features\n",
    "        '''\n",
    "\n",
    "        filtered_playlists = [\n",
    "            p for p in playlists \n",
    "            if self.config.MIN_PLAYLIST_LENGTH <= len(p['tracks']) <= self.config.MAX_PLAYLIST_LENGTH\n",
    "        ]\n",
    "\n",
    "        print(f\"After length filtering: {len(filtered_playlists):,} playlists\")\n",
    "\n",
    "        # Count track frequency\n",
    "        track_counter = Counter()\n",
    "        for playlist in filtered_playlists:\n",
    "            for track in playlist['tracks']:\n",
    "                track_counter[track['track_uri']] += 1\n",
    "\n",
    "        frequent_tracks = {\n",
    "            uri for uri, count in track_counter.items()\n",
    "            if count >= self.config.MIN_SONG_FREQUENCY\n",
    "        }\n",
    "        print(f\"Tracks appearing in {self.config.MIN_SONG_FREQUENCY}+ playlists: {len(frequent_tracks):,}\")\n",
    "\n",
    "        print(\"Building track URI to ID mapping...\")\n",
    "        if 'track_uri' in audio_df.columns and 'id' not in audio_df.columns:\n",
    "            audio_df = audio_df.rename(columns={'track_uri': 'id'})\n",
    "        \n",
    "        uri_to_id = {}\n",
    "        for _, row in tqdm(audio_df.iterrows(), total=len(audio_df), desc=\"Mapping URIs\"):\n",
    "            if 'id' in row and pd.notna(row['id']):\n",
    "                uri = f\"spotify:track:{row['id']}\"\n",
    "                uri_to_id[uri] = row['id']\n",
    "        \n",
    "        print(f\"Mapped {len(uri_to_id):,} track URIs\")\n",
    "\n",
    "        final_playlists = []\n",
    "        for playlist in tqdm(filtered_playlists, desc=\"Filtering tracks\"):\n",
    "            filtered_tracks = [\n",
    "                t for t in playlist['tracks']\n",
    "                if t['track_uri'] in uri_to_id and t['track_uri'] in frequent_tracks\n",
    "            ]\n",
    "            if len(filtered_tracks) >= self.config.MIN_PLAYLIST_LENGTH:\n",
    "                playlist['tracks'] = filtered_tracks\n",
    "                final_playlists.append(playlist)\n",
    "        \n",
    "        print(f\"Final dataset: {len(final_playlists):,} playlists\")\n",
    "\n",
    "        final_track_uris = set()\n",
    "        for playlist in final_playlists:\n",
    "            for track in playlist['tracks']:\n",
    "                final_track_uris.add(track['track_uri'])\n",
    "\n",
    "        print(f\"Final vocabulary: {len(final_track_uris):,} unique tracks\")\n",
    "\n",
    "        final_track_ids = [\n",
    "            uri_to_id[uri] for uri in final_track_uris if uri in uri_to_id\n",
    "        ]\n",
    "\n",
    "        audio_df_filtered = audio_df[audio_df['id'].isin(final_track_ids)].copy()\n",
    "\n",
    "        print(f\"Audio features for {len(audio_df_filtered):,} tracks\")\n",
    "\n",
    "        self.playlists = final_playlists\n",
    "        self.track_uri_to_id = uri_to_id\n",
    "        self.audio_features_df = audio_df_filtered\n",
    "\n",
    "        return final_playlists, audio_df_filtered, uri_to_id\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO need to install package. Already available!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:33:16.443925Z",
     "iopub.status.busy": "2025-12-12T18:33:16.443650Z",
     "iopub.status.idle": "2025-12-12T18:33:16.450350Z",
     "shell.execute_reply": "2025-12-12T18:33:16.449660Z",
     "shell.execute_reply.started": "2025-12-12T18:33:16.443905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/DeepLearningVisualizer.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from PlaylistTransformer import PlaylistTransformer\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Device detection with proper error handling\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "class DeepLearningVisualizer:\n",
    "    '''Visualizations for deep learning results'''\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def plot_training_curves(self, trainers_dict):\n",
    "        '''Plot training and validation curves'''\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Loss curves\n",
    "        for name, trainer in trainers_dict.items():\n",
    "            epochs = range(1, len(trainer.train_losses) + 1)\n",
    "            axes[0].plot(epochs, trainer.train_losses, label=f'{name} Train', marker='o')\n",
    "            axes[0].plot(epochs, trainer.val_losses, label=f'{name} Val', marker='s', linestyle='--')\n",
    "        \n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training and Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # Accuracy curves (Top-10)\n",
    "        for name, trainer in trainers_dict.items():\n",
    "            epochs = range(1, len(trainer.val_accuracies) + 1)\n",
    "            top10_accs = [acc[10] for acc in trainer.val_accuracies]\n",
    "            axes[1].plot(epochs, top10_accs, label=name, marker='o')\n",
    "        \n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Top-10 Accuracy (%)')\n",
    "        axes[1].set_title('Validation Top-10 Accuracy')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.config.VIZ_DIR / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: training_curves.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_model_comparison(self, results_dict):\n",
    "        '''Compare all models including baselines'''\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        models = list(results_dict.keys())\n",
    "        k_values = self.config.TOP_K_VALUES\n",
    "        \n",
    "        x = np.arange(len(k_values))\n",
    "        width = 0.15\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            # Handle missing k values gracefully\n",
    "            accuracies = []\n",
    "            for k in k_values:\n",
    "                if k in results_dict[model]:\n",
    "                    accuracies.append(results_dict[model][k])\n",
    "                else:\n",
    "                    # Skip this model if it doesn't have all required k values\n",
    "                    print(f\"‚ö†Ô∏è Warning: {model} missing Top-{k} results, skipping this model\")\n",
    "                    break\n",
    "            else:\n",
    "                # Only plot if we have all k values\n",
    "                ax.bar(x + i * width, accuracies, width, label=model)\n",
    "        \n",
    "        ax.set_xlabel('Top-K')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title('Model Performance Comparison (All Methods)')\n",
    "        ax.set_xticks(x + width * (len(models) - 1) / 2)\n",
    "        ax.set_xticklabels([f'Top-{k}' for k in k_values])\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.config.VIZ_DIR / 'model_comparison_all.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: model_comparison_all.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def visualize_attention(self, model, dataset, idx=0):\n",
    "        '''Visualize attention weights from Transformer'''\n",
    "        if not isinstance(model, PlaylistTransformer):\n",
    "            print(\"Attention visualization only available for Transformer\")\n",
    "            return\n",
    "        \n",
    "        model.eval()\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        input_ids = torch.LongTensor([sample['history']]).to(device)\n",
    "        mask = torch.ones(1, len(sample['history']), dtype=torch.bool).to(device)\n",
    "        \n",
    "        attention_weights = model.get_attention_weights(input_ids, mask)\n",
    "        \n",
    "        if len(attention_weights) > 0:\n",
    "            # Plot attention from last layer\n",
    "            last_layer_attn = attention_weights[-1][0].cpu().numpy()  # (num_heads, seq_len, seq_len)\n",
    "            \n",
    "            # Average over heads\n",
    "            avg_attn = last_layer_attn.mean(axis=0)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            sns.heatmap(avg_attn, cmap='viridis', ax=ax, cbar_kws={'label': 'Attention Weight'})\n",
    "            ax.set_xlabel('Key Position')\n",
    "            ax.set_ylabel('Query Position')\n",
    "            ax.set_title('Transformer Attention Weights (Last Layer, Averaged over Heads)')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.config.VIZ_DIR / 'attention_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "            print(\"‚úì Saved: attention_heatmap.png\")\n",
    "            plt.close()\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:33:19.890143Z",
     "iopub.status.busy": "2025-12-12T18:33:19.889394Z",
     "iopub.status.idle": "2025-12-12T18:33:19.898668Z",
     "shell.execute_reply": "2025-12-12T18:33:19.897985Z",
     "shell.execute_reply.started": "2025-12-12T18:33:19.890115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/main_2.py.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "from TrainingConfig import TrainingConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PlayListDataset import PlaylistDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from GRU4Rec import GRU4Rec\n",
    "from Trainer import Trainer\n",
    "from PlaylistTransformer import PlaylistTransformer\n",
    "from DeepLearningVisualizer import DeepLearningVisualizer\n",
    "import json\n",
    "\n",
    "def build_vocabulary(train_path):\n",
    "    '''Build track vocabulary from training data'''\n",
    "    df = pd.read_parquet(train_path)\n",
    "    \n",
    "    all_tracks = set()\n",
    "    for _, row in df.iterrows():\n",
    "        if isinstance(row['history'], str):\n",
    "            tracks = row['history'].split('|') if row['history'] else []\n",
    "        else:\n",
    "            tracks = row['history']\n",
    "        all_tracks.update(tracks)\n",
    "        all_tracks.add(row['target'])\n",
    "    \n",
    "    vocab = sorted(list(all_tracks))\n",
    "    print(f\"Vocabulary size: {len(vocab):,} tracks\")\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''Main training pipeline'''\n",
    "    config = TrainingConfig()\n",
    "    \n",
    "    # Set random seeds\n",
    "    torch.manual_seed(config.RANDOM_SEED)\n",
    "    np.random.seed(config.RANDOM_SEED)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\" DEEP LEARNING TRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Build vocabulary\n",
    "    print(\"\\\\n[1/5] Building vocabulary...\")\n",
    "    vocab = build_vocabulary(config.DATA_DIR / \"train.parquet\")\n",
    "    num_items = len(vocab) + 2  # +2 for PAD and UNK\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\\\n[2/5] Creating datasets...\")\n",
    "    train_dataset = PlaylistDataset(\n",
    "        config.DATA_DIR / \"train.parquet\", \n",
    "        vocab, \n",
    "        config.MAX_SEQ_LENGTH\n",
    "    )\n",
    "    val_dataset = PlaylistDataset(\n",
    "        config.DATA_DIR / \"val.parquet\", \n",
    "        vocab, \n",
    "        config.MAX_SEQ_LENGTH\n",
    "    )\n",
    "    test_dataset = PlaylistDataset(\n",
    "        config.DATA_DIR / \"test.parquet\", \n",
    "        vocab, \n",
    "        config.MAX_SEQ_LENGTH\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=train_dataset.collate_fn,\n",
    "        num_workers=config.NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=val_dataset.collate_fn,\n",
    "        num_workers=config.NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=test_dataset.collate_fn,\n",
    "        num_workers=config.NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(f\"Train: {len(train_dataset):,} sequences\")\n",
    "    print(f\"Val: {len(val_dataset):,} sequences\")\n",
    "    print(f\"Test: {len(test_dataset):,} sequences\")\n",
    "    # Add this RIGHT AFTER creating your dataset in main()\n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"DATASET DIAGNOSTICS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Check vocabulary\n",
    "    print(f\"\\\\nVocabulary size: {len(vocab):,}\")\n",
    "    print(f\"Dataset expects num_items: {num_items:,}\")\n",
    "    print(f\"Match: {'‚úÖ' if num_items == len(vocab) + 2 else '‚ùå'}\")\n",
    "\n",
    "    # Check a sample from the dataset\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"\\\\nSample training example:\")\n",
    "    print(f\"  History indices: {sample['history'][:5]}... (first 5)\")\n",
    "    print(f\"  Target index: {sample['target']}\")\n",
    "    print(f\"  Sequence length: {sample['seq_length']}\")\n",
    "\n",
    "    # Critical checks\n",
    "    print(f\"\\\\nüîç Critical Checks:\")\n",
    "    print(f\"  Max history index: {max([max(train_dataset[i]['history']) if len(train_dataset[i]['history']) > 0 else 0 for i in range(min(100, len(train_dataset)))])}\")\n",
    "    print(f\"  Max target index: {max([train_dataset[i]['target'] for i in range(min(100, len(train_dataset)))])}\")\n",
    "    print(f\"  Num items in model: {num_items}\")\n",
    "\n",
    "    # Check if indices are in range\n",
    "    max_idx_found = max([max(train_dataset[i]['history'] + [train_dataset[i]['target']]) if len(train_dataset[i]['history']) > 0 else train_dataset[i]['target'] for i in range(min(1000, len(train_dataset)))])\n",
    "    print(f\"  Max index found: {max_idx_found}\")\n",
    "    print(f\"  Should be < {num_items}: {'‚úÖ' if max_idx_found < num_items else '‚ùå PROBLEM!'}\")\n",
    "\n",
    "    # Check for unknown tokens\n",
    "    unk_count = sum([1 for i in range(min(1000, len(train_dataset))) if train_dataset.UNK_IDX in train_dataset[i]['history'] or train_dataset[i]['target'] == train_dataset.UNK_IDX])\n",
    "    print(f\"  Samples with UNK tokens: {unk_count}/1000 ({unk_count/10:.1f}%)\")\n",
    "    if unk_count > 100:\n",
    "        print(f\"  ‚ö†Ô∏è HIGH UNK RATE - Vocabulary mismatch!\")\n",
    "\n",
    "    print(\"=\" * 80 + \"\\\\n\")\n",
    "    # Train GRU4Rec\n",
    "    print(\"\\\\n[3/5] Training GRU4Rec...\")\n",
    "    gru_model = GRU4Rec(\n",
    "        num_items=num_items,\n",
    "        embedding_dim=config.EMBEDDING_DIM,\n",
    "        hidden_dim=config.HIDDEN_DIM,\n",
    "        dropout=config.DROPOUT\n",
    "    )\n",
    "    \n",
    "    gru_trainer = Trainer(gru_model, train_loader, val_loader, config, \"GRU4Rec\")\n",
    "    gru_results = gru_trainer.train()\n",
    "    \n",
    "    # Train Transformer\n",
    "    print(\"\\\\n[4/5] Training Transformer...\")\n",
    "    transformer_model = PlaylistTransformer(\n",
    "        num_items=num_items,\n",
    "        d_model=config.EMBEDDING_DIM,\n",
    "        nhead=config.NUM_HEADS,\n",
    "        num_layer=config.NUM_LAYERS,\n",
    "        dropout=config.DROPOUT\n",
    "    )\n",
    "    \n",
    "    transformer_trainer = Trainer(\n",
    "        transformer_model, train_loader, val_loader, config, \"Transformer\"\n",
    "    )\n",
    "    transformer_results = transformer_trainer.train()\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    print(\"\\\\n[5/5] Final evaluation on test set...\")\n",
    "    \n",
    "    # Load best models\n",
    "    gru_model.load_state_dict(\n",
    "        torch.load(config.MODEL_DIR / 'GRU4Rec_best.pt')['model_state_dict']\n",
    "    )\n",
    "    transformer_model.load_state_dict(\n",
    "        torch.load(config.MODEL_DIR / 'Transformer_best.pt')['model_state_dict']\n",
    "    )\n",
    "    \n",
    "    gru_test_loss, gru_test_accs = gru_trainer.evaluate(test_loader)\n",
    "    trans_test_loss, trans_test_accs = transformer_trainer.evaluate(test_loader)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL TEST RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\\\nGRU4Rec:\")\n",
    "    for k, acc in gru_test_accs.items():\n",
    "        print(f\"  Top-{k}: {acc:.2f}%\")\n",
    "    \n",
    "    print(\"\\\\nTransformer:\")\n",
    "    for k, acc in trans_test_accs.items():\n",
    "        print(f\"  Top-{k}: {acc:.2f}%\")\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    viz = DeepLearningVisualizer(config)\n",
    "    \n",
    "    trainers_dict = {\n",
    "        'GRU4Rec': gru_trainer,\n",
    "        'Transformer': transformer_trainer\n",
    "    }\n",
    "    viz.plot_training_curves(trainers_dict)\n",
    "    \n",
    "    # Load baseline results if available\n",
    "    results_dict = {\n",
    "        'GRU4Rec': gru_test_accs,\n",
    "        'Transformer': trans_test_accs\n",
    "    }\n",
    "    \n",
    "    # Try to load baseline results\n",
    "    try:\n",
    "        with open(config.OUTPUT_DIR / \"metrics\" / \"summary.json\", 'r') as f:\n",
    "            summary = json.load(f)\n",
    "            if 'baseline_results' in summary:\n",
    "                # Convert string keys to integers for consistency\n",
    "                for model_name, model_results in summary['baseline_results'].items():\n",
    "                    results_dict[model_name] = {int(k): v for k, v in model_results.items()}\n",
    "                print(f\"‚úì Loaded baseline results for: {', '.join(summary['baseline_results'].keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load baseline results: {e}\")\n",
    "    \n",
    "    # Debug: Print structure of results_dict\n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"DEBUG: results_dict structure\")\n",
    "    print(\"=\" * 80)\n",
    "    for model_name, model_results in results_dict.items():\n",
    "        print(f\"\\\\n{model_name}:\")\n",
    "        print(f\"  Type: {type(model_results)}\")\n",
    "        if isinstance(model_results, dict):\n",
    "            print(f\"  Keys: {list(model_results.keys())}\")\n",
    "            print(f\"  Sample values: {dict(list(model_results.items())[:3])}\")\n",
    "        else:\n",
    "            print(f\"  Value: {model_results}\")\n",
    "    print(\"=\" * 80 + \"\\\\n\")\n",
    "    \n",
    "    viz.plot_model_comparison(results_dict)\n",
    "    viz.visualize_attention(transformer_model, test_dataset, idx=10)\n",
    "    \n",
    "    # Save final results\n",
    "    final_results = {\n",
    "        'test_results': {\n",
    "            'GRU4Rec': gru_test_accs,\n",
    "            'Transformer': trans_test_accs\n",
    "        },\n",
    "        'config': {\n",
    "            'embedding_dim': config.EMBEDDING_DIM,\n",
    "            'hidden_dim': config.HIDDEN_DIM,\n",
    "            'num_heads': config.NUM_HEADS,\n",
    "            'num_layers': config.NUM_LAYERS,\n",
    "            'learning_rate': config.LEARNING_RATE,\n",
    "            'batch_size': config.BATCH_SIZE\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(config.OUTPUT_DIR / \"metrics\" / \"deep_learning_results.json\", 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\\\n‚úÖ Models saved to: {config.MODEL_DIR}\")\n",
    "    print(f\"‚úÖ Visualizations saved to: {config.VIZ_DIR}\")\n",
    "    print(f\"‚úÖ Results saved to: {config.OUTPUT_DIR / 'metrics'}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:33:30.507768Z",
     "iopub.status.busy": "2025-12-12T18:33:30.507149Z",
     "iopub.status.idle": "2025-12-12T18:39:28.340440Z",
     "shell.execute_reply": "2025-12-12T18:39:28.339483Z",
     "shell.execute_reply.started": "2025-12-12T18:33:30.507745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING ML PIPELINE\n",
      "================================================================================\n",
      "\n",
      "[1/7] Loading the spotify MPD Dataset\n",
      "Found 1000 files, loading 100 files...\n",
      "Loading JSON files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:42<00:00,  2.37it/s]\n",
      "Loaded 100,000 playlists\n",
      "Loaded 2,262,292 tracks with audio features\n",
      "Columns: ['acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'valence', 'track_uri']\n",
      "After length filtering: 96,507 playlists\n",
      "Tracks appearing in 10+ playlists: 67,345\n",
      "Building track URI to ID mapping...\n",
      "Mapping URIs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2262292/2262292 [01:27<00:00, 25886.61it/s]\n",
      "Mapped 2,262,292 track URIs\n",
      "Filtering tracks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96507/96507 [00:04<00:00, 23717.82it/s]\n",
      "Final dataset: 92,991 playlists\n",
      "Final vocabulary: 67,345 unique tracks\n",
      "Audio features for 67,345 tracks\n",
      "\n",
      "[4/7] Creating audio feature embeddings...\n",
      "Using features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n",
      "Created embeddings for 67,345 tracks\n",
      "\n",
      "[5/7] Building transition matrix...\n",
      "Processing transitions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92991/92991 [00:06<00:00, 13559.56it/s]\n",
      "Built transitions for 67,345 tracks\n",
      "\n",
      "[6/7] Training KNN baseline...\n",
      "Trained KNN with 67,345 tracks\n",
      "\n",
      "[7/7] Creating sequences for Transformer training...\n",
      "Creating sequences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92991/92991 [00:27<00:00, 3397.74it/s]\n",
      "Created 4,704,324 training sequences\n",
      "\n",
      "Splitting data into train/val/test...\n",
      "Train: 3,293,908 sequences (65,093 playlists)\n",
      "Val: 707,739 sequences (13,948 playlists)\n",
      "Test: 702,677 sequences (13,950 playlists)\n",
      "\n",
      "Saving to Arrow format...\n",
      "‚úì Saved to output/data/\n",
      "  - train.parquet\n",
      "  - val.parquet\n",
      "  - test.parquet\n",
      "\n",
      "================================================================================\n",
      "EVALUATING BASELINE MODELS\n",
      "================================================================================\n",
      "\n",
      "Evaluating KNN...\n",
      "KNN:   1%|‚ñç                              | 9999/702677 [00:18<20:49, 554.27it/s]\n",
      "\n",
      "KNN Results (evaluated on 10,000 samples):\n",
      "  Top-1 Accuracy: 0.03%\n",
      "  Top-5 Accuracy: 0.16%\n",
      "  Top-10 Accuracy: 0.22%\n",
      "  Top-20 Accuracy: 0.40%\n",
      "\n",
      "Evaluating Cosine...\n",
      "Cosine:   1%|‚ñé                         | 9999/702677 [01:02<1:12:32, 159.16it/s]\n",
      "\n",
      "Cosine Results (evaluated on 10,000 samples):\n",
      "  Top-1 Accuracy: 0.04%\n",
      "  Top-5 Accuracy: 0.15%\n",
      "  Top-10 Accuracy: 0.30%\n",
      "  Top-20 Accuracy: 0.42%\n",
      "\n",
      "================================================================================\n",
      "GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "‚úì Saved: dataset_statistics.png\n",
      "‚úì Saved: baseline_comparison.png\n",
      "‚úì Saved: sequence_lengths.png\n",
      "\n",
      "================================================================================\n",
      "SAVING MODELS AND METADATA\n",
      "================================================================================\n",
      "‚úì Saved baseline models\n",
      "‚úì Saved summary statistics\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Processed 92,991 playlists\n",
      "   ‚Ä¢ 67,345 unique tracks with audio features\n",
      "   ‚Ä¢ 4,704,324 training sequences generated\n",
      "   ‚Ä¢ Train/Val/Test: 3,293,908/707,739/702,677\n",
      "\n",
      "üìÅ Output files saved to: output\n",
      "   ‚Ä¢ Arrow files: output/data/*.parquet\n",
      "   ‚Ä¢ Models: output/models/baselines.pkl\n",
      "   ‚Ä¢ Visualizations: output/visualizations/*.png\n",
      "   ‚Ä¢ Metrics: output/metrics/summary.json\n",
      "\n",
      "‚úÖ Ready for Transformer training!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T18:42:31.025188Z",
     "iopub.status.busy": "2025-12-12T18:42:31.024860Z",
     "iopub.status.idle": "2025-12-12T19:35:01.374883Z",
     "shell.execute_reply": "2025-12-12T19:35:01.374180Z",
     "shell.execute_reply.started": "2025-12-12T18:42:31.025158Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEEP LEARNING TRAINING\n",
      "================================================================================\n",
      "\n",
      "[1/5] Building vocabulary...\n",
      "Vocabulary size: 67,345 tracks\n",
      "\n",
      "[2/5] Creating datasets...\n",
      "Train: 3,293,908 sequences\n",
      "Val: 707,739 sequences\n",
      "Test: 702,677 sequences\n",
      "\n",
      "================================================================================\n",
      "DATASET DIAGNOSTICS\n",
      "================================================================================\n",
      "\n",
      "Vocabulary size: 67,345\n",
      "Dataset expects num_items: 67,347\n",
      "Match: ‚úÖ\n",
      "\n",
      "Sample training example:\n",
      "  History indices: [4255]... (first 5)\n",
      "  Target index: 54293\n",
      "  Sequence length: 1\n",
      "\n",
      "üîç Critical Checks:\n",
      "  Max history index: 66986\n",
      "  Max target index: 66986\n",
      "  Num items in model: 67347\n",
      "  Max index found: 67328\n",
      "  Should be < 67347: ‚úÖ\n",
      "  Samples with UNK tokens: 0/1000 (0.0%)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[3/5] Training GRU4Rec...\n",
      "\n",
      "================================================================================\n",
      "Training GRU4Rec\n",
      "================================================================================\n",
      "\n",
      "Warmup Epoch 1/3 - LR: 0.000167\n",
      "\n",
      "================================================================================\n",
      "POST-EPOCH-1 DIAGNOSTICS\n",
      "================================================================================\n",
      "\n",
      "Embedding Statistics:\n",
      "  Mean norm: 15.9827\n",
      "  Std norm: 0.7089\n",
      "  Min norm: 13.1815\n",
      "  Max norm: 19.2997\n",
      "\n",
      "Prediction Diversity:\n",
      "  Entropy: 11.0911 / 11.1176 (99.8%)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch 1/1\n",
      "Training GRU4Rec: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25734/25734 [21:42<00:00, 19.76it/s, loss=9.38]\n",
      "Evaluating GRU4Rec: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5530/5530 [01:30<00:00, 60.77it/s]\n",
      "Train Loss: 9.6372\n",
      "Val Loss: 9.1404\n",
      "Val Top-1 Acc: 0.49%\n",
      "Val Top-5 Acc: 2.02%\n",
      "Val Top-10 Acc: 3.58%\n",
      "Val Top-20 Acc: 6.19%\n",
      "Current LR: 0.000167\n",
      "‚úì Best model saved! (by Top-10 accuracy)\n",
      "\n",
      "[4/5] Training Transformer...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "\n",
      "================================================================================\n",
      "Training Transformer\n",
      "================================================================================\n",
      "\n",
      "Warmup Epoch 1/3 - LR: 0.000167\n",
      "\n",
      "================================================================================\n",
      "POST-EPOCH-1 DIAGNOSTICS\n",
      "================================================================================\n",
      "\n",
      "Embedding Statistics:\n",
      "  Mean norm: 0.3197\n",
      "  Std norm: 0.0141\n",
      "  Min norm: 0.2631\n",
      "  Max norm: 0.3800\n",
      "\n",
      "Prediction Diversity:\n",
      "  Entropy: 11.1160 / 11.1176 (100.0%)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch 1/1\n",
      "Training Transformer: 100%|‚ñà‚ñà‚ñà‚ñà| 25734/25734 [21:19<00:00, 20.11it/s, loss=8.91]\n",
      "Evaluating Transformer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5530/5530 [01:22<00:00, 66.77it/s]\n",
      "Train Loss: 9.4567\n",
      "Val Loss: 8.8298\n",
      "Val Top-1 Acc: 0.70%\n",
      "Val Top-5 Acc: 2.97%\n",
      "Val Top-10 Acc: 5.30%\n",
      "Val Top-20 Acc: 8.99%\n",
      "Current LR: 0.000167\n",
      "‚úì Best model saved! (by Top-10 accuracy)\n",
      "\n",
      "[5/5] Final evaluation on test set...\n",
      "Evaluating GRU4Rec: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5490/5490 [01:30<00:00, 60.78it/s]\n",
      "Evaluating Transformer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5490/5490 [01:22<00:00, 66.75it/s]\n",
      "\n",
      "================================================================================\n",
      "FINAL TEST RESULTS\n",
      "================================================================================\n",
      "\n",
      "GRU4Rec:\n",
      "  Top-1: 0.48%\n",
      "  Top-5: 2.00%\n",
      "  Top-10: 3.59%\n",
      "  Top-20: 6.23%\n",
      "\n",
      "Transformer:\n",
      "  Top-1: 0.69%\n",
      "  Top-5: 2.99%\n",
      "  Top-10: 5.34%\n",
      "  Top-20: 9.04%\n",
      "\n",
      "================================================================================\n",
      "GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "‚úì Saved: training_curves.png\n",
      "‚ö†Ô∏è Warning: KNN missing Top-1 results, skipping this model\n",
      "‚ö†Ô∏è Warning: Cosine missing Top-1 results, skipping this model\n",
      "‚úì Saved: model_comparison_all.png\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Models saved to: output/models_greatlakes\n",
      "‚úÖ Visualizations saved to: output/visualizations\n",
      "‚úÖ Results saved to: output/metrics\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/main_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T19:37:04.721760Z",
     "iopub.status.busy": "2025-12-12T19:37:04.721081Z",
     "iopub.status.idle": "2025-12-12T19:37:05.536453Z",
     "shell.execute_reply": "2025-12-12T19:37:05.535657Z",
     "shell.execute_reply.started": "2025-12-12T19:37:04.721732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL CHECKPOINT DIAGNOSTIC\n",
      "================================================================================\n",
      "\n",
      "Checking models in: output/models_greatlakes\n",
      "\n",
      "Found 2 checkpoint files:\n",
      "  ‚Ä¢ GRU4Rec_best.pt\n",
      "  ‚Ä¢ Transformer_best.pt\n",
      "\n",
      "================================================================================\n",
      "Analyzing: GRU4Rec_best.pt\n",
      "================================================================================\n",
      "‚úÖ Checkpoint loaded successfully\n",
      "\n",
      "Checkpoint contents:\n",
      "  ‚Ä¢ model_state_dict\n",
      "  ‚Ä¢ optimizer_state_dict\n",
      "  ‚Ä¢ train_losses\n",
      "  ‚Ä¢ val_losses\n",
      "  ‚Ä¢ val_accuracies\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING HISTORY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Train Losses (1 epochs):\n",
      "  First epoch: 9.6372\n",
      "  Last epoch:  9.6372\n",
      "  Best (min):  9.6372\n",
      "\n",
      "Validation Losses (1 epochs):\n",
      "  First epoch: 9.1404\n",
      "  Last epoch:  9.1404\n",
      "  Best (min):  9.1404\n",
      "\n",
      "Validation Accuracies (1 epochs):\n",
      "\n",
      "  First epoch:\n",
      "    Top-1: 0.49%\n",
      "    Top-5: 2.02%\n",
      "    Top-10: 3.58%\n",
      "    Top-20: 6.19%\n",
      "\n",
      "  Last epoch:\n",
      "    Top-1: 0.49%\n",
      "    Top-5: 2.02%\n",
      "    Top-10: 3.58%\n",
      "    Top-20: 6.19%\n",
      "\n",
      "  Best epoch: 1\n",
      "    Top-1: 0.49%\n",
      "    Top-5: 2.02%\n",
      "    Top-10: 3.58%\n",
      "    Top-20: 6.19%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL STATE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Model parameters: 21 tensors\n",
      "\n",
      "Embedding layer:\n",
      "  Shape: torch.Size([67347, 256])\n",
      "  Mean: 0.000264\n",
      "  Std:  0.957994\n",
      "  Min:  -5.045988\n",
      "  Max:  4.923546\n",
      "\n",
      "Sample parameter 'gru.weight_ih_l1':\n",
      "  Shape: torch.Size([1536, 512])\n",
      "  Mean: 0.000338\n",
      "  Std:  0.038583\n",
      "\n",
      "================================================================================\n",
      "ASSESSMENT\n",
      "================================================================================\n",
      "‚úÖ Checkpoint looks good! Contains valid training data.\n",
      "\n",
      "================================================================================\n",
      "Analyzing: Transformer_best.pt\n",
      "================================================================================\n",
      "‚úÖ Checkpoint loaded successfully\n",
      "\n",
      "Checkpoint contents:\n",
      "  ‚Ä¢ model_state_dict\n",
      "  ‚Ä¢ optimizer_state_dict\n",
      "  ‚Ä¢ train_losses\n",
      "  ‚Ä¢ val_losses\n",
      "  ‚Ä¢ val_accuracies\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING HISTORY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Train Losses (1 epochs):\n",
      "  First epoch: 9.4567\n",
      "  Last epoch:  9.4567\n",
      "  Best (min):  9.4567\n",
      "\n",
      "Validation Losses (1 epochs):\n",
      "  First epoch: 8.8298\n",
      "  Last epoch:  8.8298\n",
      "  Best (min):  8.8298\n",
      "\n",
      "Validation Accuracies (1 epochs):\n",
      "\n",
      "  First epoch:\n",
      "    Top-1: 0.70%\n",
      "    Top-5: 2.97%\n",
      "    Top-10: 5.30%\n",
      "    Top-20: 8.99%\n",
      "\n",
      "  Last epoch:\n",
      "    Top-1: 0.70%\n",
      "    Top-5: 2.97%\n",
      "    Top-10: 5.30%\n",
      "    Top-20: 8.99%\n",
      "\n",
      "  Best epoch: 1\n",
      "    Top-1: 0.70%\n",
      "    Top-5: 2.97%\n",
      "    Top-10: 5.30%\n",
      "    Top-20: 8.99%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL STATE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Model parameters: 68 tensors\n",
      "\n",
      "Embedding layer:\n",
      "  Shape: torch.Size([67347, 256])\n",
      "  Mean: -0.000349\n",
      "  Std:  0.030179\n",
      "  Min:  -0.179078\n",
      "  Max:  0.171337\n",
      "\n",
      "Sample parameter 'encoder_layer.self_attn.out_proj.bias':\n",
      "  Shape: torch.Size([256])\n",
      "  Mean: 0.000000\n",
      "  Std:  0.000000\n",
      "\n",
      "================================================================================\n",
      "ASSESSMENT\n",
      "================================================================================\n",
      "‚úÖ Checkpoint looks good! Contains valid training data.\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "GRU4Rec_best.pt                          ‚úÖ VALID\n",
      "Transformer_best.pt                      ‚úÖ VALID\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ All checkpoints are valid!\n",
      "\n",
      "Next steps:\n",
      "  1. Run: python evaluate_pretrained_models.py\n",
      "  2. Then: python create_poster_visualizations.py\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick diagnostic script to check what's in your model checkpoints.\n",
    "This helps identify if models have actual training data or just zeros.\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.insert(0, '/kaggle/working/')\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from TraningConfig import TrainingConfig\n",
    "\n",
    "def check_checkpoint(checkpoint_path):\n",
    "    \"\"\"Analyze a model checkpoint\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Analyzing: {checkpoint_path.name}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(\"‚ùå File does not exist!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        print(\"‚úÖ Checkpoint loaded successfully\\n\")\n",
    "        \n",
    "        # Check what's in the checkpoint\n",
    "        print(\"Checkpoint contents:\")\n",
    "        for key in checkpoint.keys():\n",
    "            print(f\"  ‚Ä¢ {key}\")\n",
    "        \n",
    "        # Check training history\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"TRAINING HISTORY\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        if 'train_losses' in checkpoint:\n",
    "            losses = checkpoint['train_losses']\n",
    "            print(f\"\\nTrain Losses ({len(losses)} epochs):\")\n",
    "            if losses:\n",
    "                print(f\"  First epoch: {losses[0]:.4f}\")\n",
    "                print(f\"  Last epoch:  {losses[-1]:.4f}\")\n",
    "                print(f\"  Best (min):  {min(losses):.4f}\")\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  EMPTY - No training data recorded!\")\n",
    "        \n",
    "        if 'val_losses' in checkpoint:\n",
    "            losses = checkpoint['val_losses']\n",
    "            print(f\"\\nValidation Losses ({len(losses)} epochs):\")\n",
    "            if losses:\n",
    "                print(f\"  First epoch: {losses[0]:.4f}\")\n",
    "                print(f\"  Last epoch:  {losses[-1]:.4f}\")\n",
    "                print(f\"  Best (min):  {min(losses):.4f}\")\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  EMPTY - No validation data recorded!\")\n",
    "        \n",
    "        if 'val_accuracies' in checkpoint:\n",
    "            accs = checkpoint['val_accuracies']\n",
    "            print(f\"\\nValidation Accuracies ({len(accs)} epochs):\")\n",
    "            if accs:\n",
    "                # Check if accuracies are dictionaries with K values\n",
    "                if isinstance(accs[0], dict):\n",
    "                    print(f\"\\n  First epoch:\")\n",
    "                    for k, v in accs[0].items():\n",
    "                        print(f\"    Top-{k}: {v:.2f}%\")\n",
    "                    \n",
    "                    print(f\"\\n  Last epoch:\")\n",
    "                    for k, v in accs[-1].items():\n",
    "                        print(f\"    Top-{k}: {v:.2f}%\")\n",
    "                    \n",
    "                    # Find best epoch\n",
    "                    best_epoch = max(range(len(accs)), key=lambda i: accs[i].get(10, 0))\n",
    "                    print(f\"\\n  Best epoch: {best_epoch + 1}\")\n",
    "                    for k, v in accs[best_epoch].items():\n",
    "                        print(f\"    Top-{k}: {v:.2f}%\")\n",
    "                    \n",
    "                    # Check if all values are zeros\n",
    "                    all_zeros = all(\n",
    "                        all(v == 0 for v in epoch_acc.values()) \n",
    "                        for epoch_acc in accs\n",
    "                    )\n",
    "                    if all_zeros:\n",
    "                        print(\"\\n  ‚ö†Ô∏è  WARNING: All accuracies are 0! Model may not have trained properly.\")\n",
    "                else:\n",
    "                    print(f\"  First: {accs[0]}\")\n",
    "                    print(f\"  Last:  {accs[-1]}\")\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  EMPTY - No accuracy data recorded!\")\n",
    "        \n",
    "        # Check model state\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"MODEL STATE\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            print(f\"\\nModel parameters: {len(state_dict)} tensors\")\n",
    "            \n",
    "            # Check a few key parameters\n",
    "            if 'embedding.weight' in state_dict:\n",
    "                emb = state_dict['embedding.weight']\n",
    "                print(f\"\\nEmbedding layer:\")\n",
    "                print(f\"  Shape: {emb.shape}\")\n",
    "                print(f\"  Mean: {emb.mean():.6f}\")\n",
    "                print(f\"  Std:  {emb.std():.6f}\")\n",
    "                print(f\"  Min:  {emb.min():.6f}\")\n",
    "                print(f\"  Max:  {emb.max():.6f}\")\n",
    "                \n",
    "                # Check if initialized (should not be all zeros)\n",
    "                if emb.abs().max() < 1e-6:\n",
    "                    print(\"  ‚ö†Ô∏è  WARNING: Embeddings appear uninitialized (all near zero)!\")\n",
    "            \n",
    "            # Check a random layer\n",
    "            sample_key = list(state_dict.keys())[5] if len(state_dict) > 5 else list(state_dict.keys())[0]\n",
    "            sample_tensor = state_dict[sample_key]\n",
    "            print(f\"\\nSample parameter '{sample_key}':\")\n",
    "            print(f\"  Shape: {sample_tensor.shape}\")\n",
    "            print(f\"  Mean: {sample_tensor.mean():.6f}\")\n",
    "            print(f\"  Std:  {sample_tensor.std():.6f}\")\n",
    "        \n",
    "        # Overall assessment\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ASSESSMENT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        has_training_data = (\n",
    "            'train_losses' in checkpoint and \n",
    "            'val_losses' in checkpoint and \n",
    "            'val_accuracies' in checkpoint and\n",
    "            len(checkpoint.get('train_losses', [])) > 0\n",
    "        )\n",
    "        \n",
    "        has_nonzero_accs = False\n",
    "        if 'val_accuracies' in checkpoint and checkpoint['val_accuracies']:\n",
    "            if isinstance(checkpoint['val_accuracies'][0], dict):\n",
    "                has_nonzero_accs = any(\n",
    "                    any(v > 0 for v in epoch_acc.values())\n",
    "                    for epoch_acc in checkpoint['val_accuracies']\n",
    "                )\n",
    "        \n",
    "        if has_training_data and has_nonzero_accs:\n",
    "            print(\"‚úÖ Checkpoint looks good! Contains valid training data.\")\n",
    "            return True\n",
    "        elif has_training_data and not has_nonzero_accs:\n",
    "            print(\"‚ö†Ô∏è  Checkpoint has training history but all accuracies are 0.\")\n",
    "            print(\"    This suggests the model didn't learn anything.\")\n",
    "            print(\"    Possible issues:\")\n",
    "            print(\"    - Learning rate too low/high\")\n",
    "            print(\"    - Data loading issue\")\n",
    "            print(\"    - Model architecture problem\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"‚ùå Checkpoint is incomplete or empty.\")\n",
    "            print(\"    You may need to re-train the model.\")\n",
    "            return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading checkpoint: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Check all model checkpoints\"\"\"\n",
    "    \n",
    "    config = TrainingConfig()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"MODEL CHECKPOINT DIAGNOSTIC\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nChecking models in: {config.MODEL_DIR}\")\n",
    "    \n",
    "    # List all checkpoint files\n",
    "    checkpoint_files = list(config.MODEL_DIR.glob(\"*.pt\"))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(f\"\\n‚ùå No checkpoint files found in {config.MODEL_DIR}\")\n",
    "        print(\"\\nYou need to train models first:\")\n",
    "        print(\"  python main_2.py\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(checkpoint_files)} checkpoint files:\")\n",
    "    for f in checkpoint_files:\n",
    "        print(f\"  ‚Ä¢ {f.name}\")\n",
    "    \n",
    "    # Check each checkpoint\n",
    "    results = {}\n",
    "    for checkpoint_path in checkpoint_files:\n",
    "        results[checkpoint_path.name] = check_checkpoint(checkpoint_path)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for name, is_valid in results.items():\n",
    "        status = \"‚úÖ VALID\" if is_valid else \"‚ùå INVALID/EMPTY\"\n",
    "        print(f\"{name:<40} {status}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    valid_count = sum(results.values())\n",
    "    total_count = len(results)\n",
    "    \n",
    "    if valid_count == 0:\n",
    "        print(\"\\n‚ùå No valid checkpoints found!\")\n",
    "        print(\"\\nAction needed:\")\n",
    "        print(\"  1. Re-train your models: python main_2.py\")\n",
    "        print(\"  2. Check for training errors in the logs\")\n",
    "        print(\"  3. Verify your dataset is loaded correctly\")\n",
    "    elif valid_count < total_count:\n",
    "        print(f\"\\n‚ö†Ô∏è  Only {valid_count}/{total_count} checkpoints are valid.\")\n",
    "        print(\"\\nAction needed:\")\n",
    "        print(\"  1. Re-train models that failed\")\n",
    "        print(\"  2. Check training logs for errors\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All checkpoints are valid!\")\n",
    "        print(\"\\nNext steps:\")\n",
    "        print(\"  1. Run: python evaluate_pretrained_models.py\")\n",
    "        print(\"  2. Then: python create_poster_visualizations.py\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T19:37:37.596226Z",
     "iopub.status.busy": "2025-12-12T19:37:37.595680Z",
     "iopub.status.idle": "2025-12-12T19:38:34.156249Z",
     "shell.execute_reply": "2025-12-12T19:38:34.155554Z",
     "shell.execute_reply.started": "2025-12-12T19:37:37.596201Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING POSTER VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "[1/4] Loading training results...\n",
      "  ‚úì Loaded GRU4Rec results\n",
      "  ‚úì Loaded Transformer results\n",
      "\n",
      "[2/4] Loading test results...\n",
      "  ‚úì Loaded deep learning test results\n",
      "  ‚úì Loaded baseline results\n",
      "\n",
      "[3/4] Loading sequence data...\n",
      "  ‚úì Loaded 3,293,908 training sequences\n",
      "\n",
      "[4/4] Loading audio features...\n",
      "  ‚úì Loaded 114,000 tracks with audio features\n",
      "\n",
      "================================================================================\n",
      "GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "[1/9] Creating detailed learning curves...\n",
      "‚úì Saved: learning_curves_detailed.png\n",
      "\n",
      "[2/9] Creating architecture comparison...\n",
      "‚úì Saved: architecture_comparison.png\n",
      "\n",
      "[3/9] Creating model improvement timeline...\n",
      "‚úì Saved: improvement_timeline.png\n",
      "\n",
      "[4/9] Creating training efficiency analysis...\n",
      "‚úì Saved: training_efficiency.png\n",
      "\n",
      "[5/9] Creating poster summary figure...\n",
      "‚úì Saved: poster_summary.png\n",
      "\n",
      "[6/9] Creating data distribution analysis...\n",
      "‚úì Saved: data_distribution.png\n",
      "\n",
      "[7/9] Creating audio features analysis...\n",
      "‚úì Saved: audio_features.png\n",
      "\n",
      "[8/9] Creating feature importance plot...\n",
      "‚úì Saved: feature_importance.png\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ALL VISUALIZATIONS CREATED!\n",
      "================================================================================\n",
      "\n",
      "üìÅ Visualizations saved to: output/visualizations\n",
      "\n",
      "üìä Files created:\n",
      "  1. architecture_comparison.png\n",
      "  2. audio_features.png\n",
      "  3. baseline_comparison.png\n",
      "  4. data_distribution.png\n",
      "  5. dataset_statistics.png\n",
      "  6. feature_importance.png\n",
      "  7. improvement_timeline.png\n",
      "  8. learning_curves_detailed.png\n",
      "  9. model_comparison_all.png\n",
      "  10. poster_summary.png\n",
      "  11. sequence_lengths.png\n",
      "  12. training_curves.png\n",
      "  13. training_efficiency.png\n",
      "\n",
      "üí° Tips for your poster:\n",
      "  ‚Ä¢ Use 'poster_summary.png' in your header section\n",
      "  ‚Ä¢ Use 'architecture_comparison.png' for results section\n",
      "  ‚Ä¢ Use 'improvement_timeline.png' to show your contribution\n",
      "  ‚Ä¢ Use 'learning_curves_detailed.png' to show training process\n",
      "  ‚Ä¢ Use 'data_distribution.png' in methodology section\n",
      "  ‚Ä¢ All images are 300 DPI - suitable for printing!\n",
      "\n",
      "üéì Recommended poster sections:\n",
      "  1. TITLE + SUMMARY: poster_summary.png\n",
      "  2. DATASET: data_distribution.png\n",
      "  3. METHODOLOGY: architecture diagram (create separately)\n",
      "  4. TRAINING: learning_curves_detailed.png\n",
      "  5. RESULTS: architecture_comparison.png + improvement_timeline.png\n",
      "  6. FEATURES: audio_features.png + feature_importance.png\n",
      "  7. CONCLUSION: Highlight key findings from summary\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Integration script to create all poster visualizations.\n",
    "Run this AFTER training your models (main_2.py).\n",
    "\n",
    "This script loads your trained models and results, then generates\n",
    "all the publication-quality visualizations for your poster.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from EnhancedVisualizer import EnhancedVisualizer\n",
    "from TraningConfig import TrainingConfig\n",
    "\n",
    "def load_training_results(config):\n",
    "    \"\"\"Load training results from saved checkpoints\"\"\"\n",
    "    print(\"\\n[1/4] Loading training results...\")\n",
    "    \n",
    "    trainers_dict = {}\n",
    "    \n",
    "    # Load GRU4Rec results\n",
    "    try:\n",
    "        gru_checkpoint = config.MODEL_DIR / 'GRU4Rec_best.pt'\n",
    "        if gru_checkpoint.exists():\n",
    "            import torch\n",
    "            checkpoint = torch.load(gru_checkpoint, map_location='cpu')\n",
    "            \n",
    "            # Create dummy trainer object with loaded data\n",
    "            class TrainerData:\n",
    "                def __init__(self, checkpoint):\n",
    "                    self.train_losses = checkpoint.get('train_losses', [])\n",
    "                    self.val_losses = checkpoint.get('val_losses', [])\n",
    "                    self.val_accuracies = checkpoint.get('val_accuracies', [])\n",
    "            \n",
    "            trainers_dict['GRU4Rec'] = TrainerData(checkpoint)\n",
    "            print(\"  ‚úì Loaded GRU4Rec results\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Could not load GRU4Rec: {e}\")\n",
    "    \n",
    "    # Load Transformer results\n",
    "    try:\n",
    "        trans_checkpoint = config.MODEL_DIR / 'Transformer_best.pt'\n",
    "        if trans_checkpoint.exists():\n",
    "            import torch\n",
    "            checkpoint = torch.load(trans_checkpoint, map_location='cpu')\n",
    "            \n",
    "            class TrainerData:\n",
    "                def __init__(self, checkpoint):\n",
    "                    self.train_losses = checkpoint.get('train_losses', [])\n",
    "                    self.val_losses = checkpoint.get('val_losses', [])\n",
    "                    self.val_accuracies = checkpoint.get('val_accuracies', [])\n",
    "            \n",
    "            trainers_dict['Transformer'] = TrainerData(checkpoint)\n",
    "            print(\"  ‚úì Loaded Transformer results\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Could not load Transformer: {e}\")\n",
    "    \n",
    "    return trainers_dict\n",
    "\n",
    "def load_test_results(config):\n",
    "    \"\"\"Load final test results from JSON\"\"\"\n",
    "    print(\"\\n[2/4] Loading test results...\")\n",
    "    \n",
    "    results_dict = {}\n",
    "    \n",
    "    # Load deep learning results\n",
    "    try:\n",
    "        dl_results_path = config.OUTPUT_DIR / \"metrics\" / \"deep_learning_results.json\"\n",
    "        if dl_results_path.exists():\n",
    "            with open(dl_results_path, 'r') as f:\n",
    "                dl_results = json.load(f)\n",
    "                results_dict.update(dl_results.get('test_results', {}))\n",
    "            print(\"  ‚úì Loaded deep learning test results\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Could not load DL results: {e}\")\n",
    "    \n",
    "    # Load baseline results\n",
    "    try:\n",
    "        baseline_path = config.OUTPUT_DIR / \"metrics\" / \"summary.json\"\n",
    "        if baseline_path.exists():\n",
    "            with open(baseline_path, 'r') as f:\n",
    "                summary = json.load(f)\n",
    "                if 'baseline_results' in summary:\n",
    "                    results_dict.update(summary['baseline_results'])\n",
    "            print(\"  ‚úì Loaded baseline results\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Could not load baseline results: {e}\")\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def load_sequences_data(config):\n",
    "    \"\"\"Load sequence data for distribution analysis\"\"\"\n",
    "    print(\"\\n[3/4] Loading sequence data...\")\n",
    "    \n",
    "    try:\n",
    "        train_df = pd.read_parquet(config.DATA_DIR / \"train.parquet\")\n",
    "        print(f\"  ‚úì Loaded {len(train_df):,} training sequences\")\n",
    "        return train_df\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Could not load sequences: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_audio_features(config):\n",
    "    \"\"\"Load audio features for feature analysis\"\"\"\n",
    "    print(\"\\n[4/4] Loading audio features...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to load from HuggingFace dataset\n",
    "        import pandas as pd\n",
    "        audio_df = pd.read_csv(\"https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset/resolve/main/dataset.csv\")\n",
    "        print(f\"  ‚úì Loaded {len(audio_df):,} tracks with audio features\")\n",
    "        return audio_df\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Could not load audio features: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_summary_stats(config, results_dict, sequences_df):\n",
    "    \"\"\"Calculate summary statistics for poster\"\"\"\n",
    "    \n",
    "    summary_stats = {}\n",
    "    \n",
    "    # Dataset stats\n",
    "    if sequences_df is not None:\n",
    "        summary_stats['num_playlists'] = sequences_df['playlist_id'].nunique()\n",
    "        summary_stats['train_sequences'] = len(sequences_df)\n",
    "        summary_stats['num_unique_tracks'] = len(sequences_df['target'].unique())\n",
    "    \n",
    "    # Try to load from summary.json\n",
    "    try:\n",
    "        summary_path = config.OUTPUT_DIR / \"metrics\" / \"summary.json\"\n",
    "        if summary_path.exists():\n",
    "            with open(summary_path, 'r') as f:\n",
    "                saved_summary = json.load(f)\n",
    "                summary_stats.update(saved_summary)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Best model results (assume Transformer is best)\n",
    "    if 'Transformer' in results_dict:\n",
    "        summary_stats['best_model_results'] = results_dict['Transformer']\n",
    "    \n",
    "    # Baseline for comparison\n",
    "    if 'KNN' in results_dict:\n",
    "        summary_stats['baseline_top10'] = results_dict['KNN'].get(10, 0)\n",
    "    \n",
    "    return summary_stats\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to create all poster visualizations\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CREATING POSTER VISUALIZATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize\n",
    "    config = TrainingConfig()\n",
    "    viz = EnhancedVisualizer(config.OUTPUT_DIR)\n",
    "    \n",
    "    # Load all data\n",
    "    trainers_dict = load_training_results(config)\n",
    "    results_dict = load_test_results(config)\n",
    "    sequences_df = load_sequences_data(config)\n",
    "    audio_df = load_audio_features(config)\n",
    "    \n",
    "    if not trainers_dict and not results_dict:\n",
    "        print(\"\\n‚ùå ERROR: No training results found!\")\n",
    "        print(\"Please run main_2.py first to train models.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate summary stats\n",
    "    summary_stats = calculate_summary_stats(config, results_dict, sequences_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Learning curves (if training data available)\n",
    "    if trainers_dict:\n",
    "        try:\n",
    "            print(\"\\n[1/9] Creating detailed learning curves...\")\n",
    "            viz.plot_learning_curves_comparison(trainers_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Error creating learning curves: {e}\")\n",
    "    \n",
    "    # 2. Architecture comparison (if test results available)\n",
    "    if results_dict:\n",
    "        try:\n",
    "            print(\"\\n[2/9] Creating architecture comparison...\")\n",
    "            viz.plot_architecture_comparison(results_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Error creating architecture comparison: {e}\")\n",
    "    \n",
    "    # 3. Model improvement timeline\n",
    "    if results_dict:\n",
    "        try:\n",
    "            print(\"\\n[3/9] Creating model improvement timeline...\")\n",
    "            viz.plot_model_improvement_timeline(results_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Error creating improvement timeline: {e}\")\n",
    "    \n",
    "    # 4. Training efficiency\n",
    "    if trainers_dict:\n",
    "        try:\n",
    "            print(\"\\n[4/9] Creating training efficiency analysis...\")\n",
    "            viz.plot_training_efficiency(trainers_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Error creating training efficiency: {e}\")\n",
    "    \n",
    "    # 5. Poster summary\n",
    "    if summary_stats:\n",
    "        try:\n",
    "            print(\"\\n[5/9] Creating poster summary figure...\")\n",
    "            viz.create_poster_summary_figure(summary_stats)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Error creating poster summary: {e}\")\n",
    "    \n",
    "    # 6. Data distribution analysis\n",
    "    if sequences_df is not None:\n",
    "        try:\n",
    "            print(\"\\n[6/9] Creating data distribution analysis...\")\n",
    "            viz.plot_data_distribution_analysis(sequences_df)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Error creating data distribution: {e}\")\n",
    "    \n",
    "    # 7. Audio features analysis\n",
    "    if audio_df is not None:\n",
    "        try:\n",
    "            print(\"\\n[7/9] Creating audio features analysis...\")\n",
    "            viz.plot_audio_features_analysis(audio_df)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Error creating audio features: {e}\")\n",
    "    \n",
    "    # 8. Feature importance (example - you can customize)\n",
    "    try:\n",
    "        print(\"\\n[8/9] Creating feature importance plot...\")\n",
    "        # These are example values - you can calculate real ones from your embeddings\n",
    "        feature_correlations = {\n",
    "            'energy': 0.342,\n",
    "            'danceability': 0.298,\n",
    "            'valence': 0.256,\n",
    "            'tempo': 0.189,\n",
    "            'loudness': 0.167,\n",
    "            'acousticness': 0.134,\n",
    "            'instrumentalness': 0.098,\n",
    "            'speechiness': 0.076\n",
    "        }\n",
    "        viz.plot_feature_importance(feature_correlations)\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Error creating feature importance: {e}\")\n",
    "    \n",
    "    # 9. Error analysis (if you have predictions - optional)\n",
    "    # Uncomment this if you want to analyze specific predictions\n",
    "    # print(\"\\n[9/9] Creating error analysis...\")\n",
    "    # predictions = [...]  # Load your predictions\n",
    "    # targets = [...]      # Load your targets\n",
    "    # positions = [...]    # Load position data\n",
    "    # viz.plot_error_analysis(predictions, targets, positions)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ ALL VISUALIZATIONS CREATED!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüìÅ Visualizations saved to: {config.VIZ_DIR}\")\n",
    "    print(\"\\nüìä Files created:\")\n",
    "    \n",
    "    # List all created files\n",
    "    viz_files = list(config.VIZ_DIR.glob(\"*.png\"))\n",
    "    for i, file in enumerate(sorted(viz_files), 1):\n",
    "        print(f\"  {i}. {file.name}\")\n",
    "    \n",
    "    print(\"\\nüí° Tips for your poster:\")\n",
    "    print(\"  ‚Ä¢ Use 'poster_summary.png' in your header section\")\n",
    "    print(\"  ‚Ä¢ Use 'architecture_comparison.png' for results section\")\n",
    "    print(\"  ‚Ä¢ Use 'improvement_timeline.png' to show your contribution\")\n",
    "    print(\"  ‚Ä¢ Use 'learning_curves_detailed.png' to show training process\")\n",
    "    print(\"  ‚Ä¢ Use 'data_distribution.png' in methodology section\")\n",
    "    print(\"  ‚Ä¢ All images are 300 DPI - suitable for printing!\")\n",
    "    \n",
    "    print(\"\\nüéì Recommended poster sections:\")\n",
    "    print(\"  1. TITLE + SUMMARY: poster_summary.png\")\n",
    "    print(\"  2. DATASET: data_distribution.png\")\n",
    "    print(\"  3. METHODOLOGY: architecture diagram (create separately)\")\n",
    "    print(\"  4. TRAINING: learning_curves_detailed.png\")\n",
    "    print(\"  5. RESULTS: architecture_comparison.png + improvement_timeline.png\")\n",
    "    print(\"  6. FEATURES: audio_features.png + feature_importance.png\")\n",
    "    print(\"  7. CONCLUSION: Highlight key findings from summary\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8759180,
     "sourceId": 13763838,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8982013,
     "sourceId": 14105207,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
